{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d4ea3c-2d2b-48a3-a388-096f40f14a23",
   "metadata": {},
   "source": [
    "# Train word2vec from scratch\n",
    "\n",
    "### Reasons for \n",
    "- `word2vec`: it is more **robust** to changes in small corpora compared to other standard embedding methods.\n",
    "- `skip-gram architecture`: skip-gram predicts neighboring words based on the central word. Therefore, it is much better at capturing **semantic relationships**.\n",
    "- `training from scrach`: unlike fine-tuning a pre-trained model, it can reflect the co-occurrence pattern **solely in the generated datasets** without introducing the existing pattern from pre-trained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08b7b4c-9347-43a1-b099-054a753b527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09eb40-e26b-4c6f-a89a-7cd6e5b172f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define global parameters\n",
    "\n",
    "After preprocessing, texts are stored in .txt files separately for each gender. Since ChatGPT is frequently either \"at capacity\" or \"having too many requests in 1 hour\", and its connection with ChatGPT Wrapper is sometimes unstable, data collection can be extremely slow. Therefore, the model training was divided into four sessions with 300 new stories each. Stories were broken down into tokenized sentences and put together for random shuffling.\n",
    "\n",
    "- PATH: Location of the files.\n",
    "- FILE_NAMES: A list of file names with 100 stories for every gender.\n",
    "- TRAIN_MODEL_NAME: If this is not the initial model for the first 300 stories, use it to load a previous model.\n",
    "- SAVE_MODEL_NAME: The model to be saved after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bb3b0d-6f0f-4f55-ab0f-45d5281da59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''\n",
    "# FILE_NAMES = ['h1-100', 's1-100', 't1-100']\n",
    "# FILE_NAMES = ['h101-200', 's101-200', 't101-200']\n",
    "# FILE_NAMES = ['h201-300', 's201-300', 't201-300']\n",
    "FILE_NAMES = ['h301-400', 's301-400', 't301-400']\n",
    "\n",
    "TRAINED_MODEL_NAME = '3rd_300_hst'           # False\n",
    "SAVE_MODEL_NAME = '4th_400_hst'              # '1st_100_hst', '2nd_200_hst', '3rd_300_hst', '4th_400_hst'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9cb2c9-78c6-42bc-a649-e986340e9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for f_name in FILE_NAMES:\n",
    "    with open(PATH + f_name +'.txt') as f:\n",
    "        file = f.read().replace('\\n','').split('.')\n",
    "        data += file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6d9374-8ae2-4f2a-a86b-e616bd63687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = [word_tokenize(i) for i in data]\n",
    "random.shuffle(tok)\n",
    "# len(tok)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f23f741-f75c-4314-8ca8-1dfc52f7b1e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A summary of input stories \n",
    "\n",
    "It's also necessary and convenient to have some basic descriptive statistics before training. The 1200 stories contain **18461 sentences with 337244 words**."
   ]
  },
  {
   "cell_type": "raw",
   "id": "847faef0-6941-4c5f-a451-1fdb207a8a37",
   "metadata": {},
   "source": [
    "83160 + 84867 + 85326 + 83891 = 337244 words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31821edf-8ac9-426d-bac4-285dab1c73e7",
   "metadata": {},
   "source": [
    "4563 + 4621 + 4663 + 4614 = 18461 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3985a55-2e93-4eca-acbd-febdd5fe502b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training and saving the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7790c31f-83ee-4e2e-904b-0d47ba2d9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINED_MODEL_NAME:\n",
    "    model = Word2Vec.load(f\"{TRAINED_MODEL_NAME}.model\")\n",
    "    if model.epochs == 5:\n",
    "        model.train(tok, total_examples=len(tok), epochs=model.epochs)\n",
    "else:\n",
    "    model = Word2Vec(sentences=tok, vector_size=100, min_count=1, sg=1, workers=4, epochs=5)\n",
    "\n",
    "model.save(f\"{SAVE_MODEL_NAME}.model\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
